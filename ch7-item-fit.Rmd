---
title: Chapter 7
author: "BRT"
date: "`r format(Sys.Date(), '%m-%d-%Y')`"
header-includes:
  - \usepackage{booktabs}
  - \usepackage{float}
  - \usepackage{tabu}
output:
  html_document:
    toc: yes
    toc_depth: 5
    toc_float: true
---

# Item Parameters and Item Fit

```{r include=FALSE}
knitr::opts_chunk$set(echo = FALSE, results = "asis")

require(knitr)
require(kableExtra)
require(here)
library(purrr)
# Two folders needed under 'data' folder

# /data/ifit, this can be found at this link 
# /datastore/2020PASA_ScalingAnalysis/ifit

# /data/item-estimates, thi can be found at this link 
# /datastore/2020PASA_ScalingAnalysis/par-estimates-2020/item-estimates


p <- fs::dir_ls(here::here("data", "ifit"))

names <- list.files(here::here("data", "ifit"))

ela <- names[grep('ela',names)]
ela <- ela[c(3:14,1:2)]
math <- names[grep('math',names)]
math <- math[c(3:14,1:2)]
science <- names[grep('science',names)]
science <- science[c(3:6,1:2)]
```

## IRT Models for Scaling

### ELA & Mathematics: 2PL IRT

$$P(Y=1)=\frac{e^{a_j(\theta_i - b_j)}}{1+e^{a_j(\theta_i - b_j)}}$$

### Science: 1PL IRT w/ Guessing 

$$P(Y=1|\theta_i,a, b_j,c_j)=c_j + (1-c_j)\left (\frac{e^{a(\theta_i - b_j)}}{1+e^{a(\theta_i - b_j)}}  \right )$$


## Item Parameter and Fit Summary
### ELA

Item parameters and fit statistics are summarized by content area, grade, and tier for all PASA tests in the tables, below. RMSEA, the root mean square error of approximation, offers an indication of item fit to the two-parameter logistic (2PL) IRT models used in scaling ELA and Math, and the one-parameter logistic (2PL) IRT model with guessing parameter used in scaling Science. For RMSEA, values of < 0.03 indicate excellent fit, with values < 0.07 (see Steigler, 2007). Below, RMSEA item fit statistics are summarized, with the range, mean, and potentially problematic items (RMSEA $\geq$ 0.08) listed, for each ELA test. RMSEA for all ELA items are then given in the grade- and tier-level tables that follow:

- **Grade 3, Tier 1:** *Range* = 0.00 to 0.06; *M* = 0.017
- **Grade 3, Tier 2:** *Range* = 0.00 to 0.14; *M* = 0.045; E03AV4.1.1a.10, E03AV4.1.1a.12, E03AV4.1.1b.4, E03AV4.1.2a.7, and E03BK1.1.2b.7; RMSEA for item E03C1.3.1a.2 was not estimable
- **Grade 4, Tier 1:** *Range* = 0.00 to 0.04; *M* = 0.015
- **Grade 4, Tier 2:** *Range* = 0.00 to 0.06; *M* = 0.017
- **Grade 5, Tier 1:** *Range* = 0.00 to 0.08; *M* = 0.014; E05BK1.1.1c.4
- **Grade 5, Tier 2:** *Range* = 0.00 to 0.05; *M* = 0.012
- **Grade 6, Tier 1:** *Range* = 0.00 to 0.07; *M* = 0.019
- **Grade 6, Tier 2:** *Range* = 0.00 to 0.04; *M* = 0.014
- **Grade 7, Tier 1:** *Range* = 0.00 to 0.13; *M* = 0.025; E07AK1.1.1c.3 
- **Grade 7, Tier 2:** *Range* = 0.00 to 0.09; *M* = 0.025; E07AK1.1.1a.8 and E07AV4.1.1a.9
- **Grade 8, Tier 1:** *Range* = 0.00 to 0.08; *M* = 0.020; E08BK1.1.2a.7
- **Grade 8, Tier 2:** *Range* = 0.00 to 0.04; *M* = 0.016
- **Grade 11, Tier 1:** *Range* = 0.00 to 0.04; *M* = 0.013
- **Grade 11, Tier 2:** *Range* = 0.00 to 0.05; *M* = 0.013

```{r, eval=TRUE,echo=FALSE, results='asis'}
ifit_e <- vector('list', length(ela)) 
for (i in 1:length(ela)) {
  ifit_e[[i]] = read.csv(here::here("data", "ifit", ela[i]))
  ifit_e[[i]] = ifit_e[[i]][,c(1,2,3,5,4)]
  colnames(ifit_e[[i]]) <- c('Item Label','S_X2','Df','p-value','RMSEA')
  ifit_e[[i]][,4] = round(ifit_e[[i]][,4],3)
  ifit_e[[i]][which(ifit_e[[i]][,4] < .002),4] <- 'p<.002'
  ifit_e[[i]][,5] = round(ifit_e[[i]][,5],3)
  
  ipar <- read.csv(here::here("data","item-estimates",ela[[i]]))
  ipar[,1] <- gsub("-",".",ipar[,1])
  colnames(ipar)[1] = "Item Label"
  ifit_e[[i]] = merge(ifit_e[[i]],ipar[,c("Item Label","anchor","a_estimate","b_estimate","g_estimate")])
  ifit_e[[i]] = ifit_e[[i]][,c(1,6,7,8,9,2,3,4,5)]
  
  colnames(ifit_e[[i]])[c(2,3,4,5)] = c("Anchor","Discrimination","Difficulty","Guessing") 
}

ind1 <- c(3,3,4,4,5,5,6,6,7,7,8,8,11,11)
ind2 <- c(1,2,1,2,1,2,1,2,1,2,1,2,1,2)

```

```{r eval = knitr::is_latex_output()}
pwalk(list(ind1, ind2, ifit_e), ~{ 
  cat("\n",
      paste("### Grade", ..1, "Tier", ..2),
      sep = "\n")
  
  ..3 %>% 
    kable("latex", digits = 2, booktabs = TRUE, row.names = FALSE) %>%
    kable_styling(full_width = FALSE) %>% 
    print()
})
```

```{r eval = knitr::is_html_output()}
pwalk(list(ind1, ind2, ifit_e), ~{ 
  cat("<br/>")
  cat("\n",
      paste("### Grade", ..1, "Tier", ..2),
      sep = "\n")
  
  print(gt::gt(..3))
})
```

```{r eval = knitr::is_latex_output(), results = "asis"}
cat("\\newpage")
```


### Math

RMSEA item fit statistics are summarized, with the range, mean, and potentially problematic items (RMSEA $\geq$ 0.08) listed, for each Math test. RMSEA for all Math items are then given in the grade- and tier-level tables that follow:

- **Grade 3, Tier 1:** *Range* = 0.00 to 0.04; *M* = 0.018
- **Grade 3, Tier 2:** *Range* = 0.00 to 0.09; *M* = 0.037; M03BO3.1.5b.3 and M03DM1.2.1a.5
- **Grade 4, Tier 1:** *Range* = 0.00 to 0.04; *M* = 0.017
- **Grade 4, Tier 2:** *Range* = 0.00 to 0.18; *M* = 0.038; M04AT1.1.3a.2 and M04CG1.1.2a.6
- **Grade 5, Tier 1:** *Range* = 0.00 to 0.04; *M* = 0.020
- **Grade 5, Tier 2:** *Range* = 0.00 to 0.06; *M* = 0.024
- **Grade 6, Tier 1:** *Range* = 0.00 to 0.06; *M* = 0.020
- **Grade 6, Tier 2:** *Range* = 0.00 to 0.12; *M* = 0.026; M06AR112a_2019.1 and M06DS1.1.2a.11
- **Grade 7, Tier 1:** *Range* = 0.00 to 0.07; *M* = 0.027
- **Grade 7, Tier 2:** *Range* = 0.00 to 0.07; *M* = 0.020
- **Grade 8, Tier 1:** *Range* = 0.00 to 0.12; *M* = 0.028; M08BF2.1.1a.22 
- **Grade 8, Tier 2:** *Range* = 0.00 to 0.05; *M* = 0.015
- **Grade 11, Tier 1:** *Range* = 0.00 to 0.08; *M* = 0.027; CC24HSB5a_2019M.3
- **Grade 11, Tier 2:** *Range* = 0.00 to 0.07; *M* = 0.013; RMSEA for items CC.2.3.HSA13a.15 and CC23HSA13a_2019.1 were not estimable

```{r, eval=TRUE,echo=FALSE, results='asis'}

# setwd('B:/Outside Consulting/PASA/pasa-analyses-2020/data/ifit')

ifit_m <- vector('list',length(math)) 

for (i in 1:length(math)) {
  ifit_m[[i]] = read.csv(here::here("data", "ifit", math[i]))
  ifit_m[[i]] = ifit_m[[i]][,c(1,2,3,5,4)]
  colnames(ifit_m[[i]]) <- c('Item Label','S_X2','Df','p-value','RMSEA')
  ifit_m[[i]][,4] = round(ifit_m[[i]][,4],3)
  ifit_m[[i]][which(ifit_m[[i]][,4] < .002),4] <- 'p<.002'
  ifit_m[[i]][,5] = round(ifit_m[[i]][,5],3)
  
  ipar <- read.csv(here::here("data","item-estimates",math[[i]]))
  ipar[,1] <- gsub("-",".",ipar[,1])
  colnames(ipar)[1] = "Item Label"
  ifit_m[[i]] = merge(ifit_m[[i]],ipar[,c("Item Label","anchor","a_estimate","b_estimate","g_estimate")])
  ifit_m[[i]] = ifit_m[[i]][,c(1,6,7,8,9,2,3,4,5)]
  
  colnames(ifit_m[[i]])[c(2,3,4,5)] = c("Anchor","Discrimination","Difficulty","Guessing") 

}

ind1 <- c(3,3,4,4,5,5,6,6,7,7,8,8,11,11)
ind2 <- c(1,2,1,2,1,2,1,2,1,2,1,2,1,2)
```


```{r eval = knitr::is_latex_output()}
pwalk(list(ind1, ind2, ifit_m), ~{ 
  cat("\n",
      paste("### Grade", ..1, "Tier", ..2),
      sep = "\n")
  
  ..3 %>% 
    kable("latex", digits = 2, booktabs = TRUE, row.names = FALSE) %>%
    kable_styling(full_width = FALSE) %>% 
    print()
})
```

```{r eval = knitr::is_html_output()}
pwalk(list(ind1, ind2, ifit_m), ~{ 
  cat("\n",
      paste("### Grade", ..1, "Tier", ..2),
      sep = "\n")
  
  print(gt::gt(..3))
})
```

```{r eval = knitr::is_latex_output(), results = "asis"}
cat("\\newpage")
```

### Science

RMSEA item fit statistics are summarized, with the range, mean, and potentially problematic items (RMSEA $\geq$ 0.08) listed, for each Science test. RMSEA for all Science items are then given in the grade- and tier-level tables that follow:

- **Grade 4, Tier 1:** *Range* = 0.00 to 0.10; *M* = 0.039; S4.B.1.1.3.3 and S4.B.1.1.4.4
- **Grade 4, Tier 2:** *Range* = 0.00 to 0.15; *M* = 0.033; S4.A.2.2.1.4 and S4.D.1.2.2.3
- **Grade 8, Tier 1:** *Range* = 0.00 to 0.08; *M* = 0.035; S8.A.3.1.5b.2
- **Grade 8, Tier 2:** *Range* = 0.00 to 0.12; *M* = 0.032; S8.A.2.2.1.6, S8.B.1.1.3.5, and S8.C.2.2.3.6
- **Grade 11, Tier 1:** *Range* = 0.00 to 0.12; *M* = 0.049; S11.A.2.1.3.6, S11.A.3.1.2.7, S11.B.3.1.4a.2, S11.B.3.1.4b.11, and S11.B.3.1.4b.9
- **Grade 11, Tier 2:** *Range* = 0.00 to 0.06; *M* = 0.023; RMSEA for item S11.B.1.1.2.6 was not estimable

```{r, eval=TRUE,echo=FALSE, results='asis'}

# setwd('B:/Outside Consulting/PASA/pasa-analyses-2020/data/ifit')

ifit_s <- vector('list',length(science)) 

for (i in 1:length(science)) {
  ifit_s[[i]] = read.csv(here::here("data", "ifit", science[i]))
  ifit_s[[i]] = ifit_s[[i]][,c(1,2,3,5,4)]
  colnames(ifit_s[[i]]) <- c('Item Label','S_X2','Df','p-value','RMSEA')
  ifit_s[[i]][,4] = round(ifit_s[[i]][,4],3)
  ifit_s[[i]][which(ifit_s[[i]][,4] < .002),4] <- 'p<.002'
  ifit_s[[i]][,5] = round(ifit_s[[i]][,5],3)
  
  ipar <- read.csv(here::here("data","item-estimates",science[[i]]))
  ipar[,1] <- gsub("-",".",ipar[,1])
  colnames(ipar)[1] = "Item Label"
  ifit_s[[i]] = merge(ifit_s[[i]],ipar[,c("Item Label","anchor","a_estimate","b_estimate","g_estimate")])
  ifit_s[[i]] = ifit_s[[i]][,c(1,6,7,8,9,2,3,4,5)]
  
  colnames(ifit_s[[i]])[c(2,3,4,5)] = c("Anchor","Discrimination","Difficulty","Guessing") 

}

ind1 <- c(4,4,8,8,11,11)
ind2 <- c(1,2,1,2,1,2)
```


```{r eval = knitr::is_latex_output(), results = "asis"}
pwalk(list(ind1, ind2, ifit_s), ~{ 
  cat("\n",
      paste("### Grade", ..1, "Tier", ..2),
      sep = "\n")
  
  ..3 %>% 
    kable("latex", digits = 2, booktabs = TRUE, row.names = FALSE) %>%
    kable_styling(full_width = FALSE) %>% 
    print()
})
```

```{r eval = knitr::is_html_output(), results = "asis"}
pwalk(list(ind1, ind2, ifit_s), ~{ 
  cat("\n",
      paste("### Grade", ..1, "Tier", ..2),
      sep = "\n")
  
  print(gt::gt(..3))
})
```


Steiger, J. H. (2007), Understanding the limitations of global fit assessment in structural equation modeling. *Personality and Individual Differences*, *42*, 893-98.


```{r, include=knitr::is_html_output(), results = "asis"}
cat('
<div class="tocify-extend-page" data-unique="tocify-extend-page" style="height: 0;"></div>
'
)
```